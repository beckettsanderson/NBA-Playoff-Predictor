{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1303d8ec",
   "metadata": {},
   "source": [
    "# Data Scraping and Cleaning\n",
    "\n",
    "**Names:** Beckett Sanderson, Marcos Equiza Gasco, Sean Ediger\n",
    "\n",
    "**Date**: 04/19/2023\n",
    "\n",
    "The scraping of our data took a lot of work so we created a file to scrape all the data we wanted and save it as a csv so we wouldn't have to run it each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7586cc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "MIN_YEAR = 2010\n",
    "MAX_YEAR = 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f6e33",
   "metadata": {},
   "source": [
    "### Scrape Playoff Data\n",
    "A key part of our model was predicting how far in the playoffs the teams would go. This was in a separate table that we had to scrape and sort. The site contained all the playoff data since the beginning of the NBA so we had to remove the years we didn't want to track. There were also many rows and columns that were empty or contained headers so we had to remove those values as well. Finally we adjusted the team names to line up with what our team data would have and set the playoff success to the numeric values we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aa9fe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_playoffs(min_year):\n",
    "    \"\"\"\n",
    "    Load in playoff dataset and clean it to apply to our teams data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    min_year : int\n",
    "        the lowest year to collect playoff data from\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_playoffs : DataFrame\n",
    "        cleaned df containing the year, round, and teams for each playoff series\n",
    "\n",
    "    \"\"\"\n",
    "    # read in the playoff data\n",
    "    playoff_url = \"https://www.basketball-reference.com/playoffs/series.html\"\n",
    "    df_playoffs = pd.read_html(playoff_url, header=1)[0]\n",
    "\n",
    "    # remove empty rows and rows with labels\n",
    "    df_playoffs.dropna(axis=0, how='all', inplace=True)\n",
    "    df_playoffs.drop(df_playoffs[df_playoffs.Yr == 'Yr'].index, inplace=True)\n",
    "\n",
    "    # drop years below lowest year we're using\n",
    "    df_playoffs = df_playoffs.astype({'Yr': 'int'})\n",
    "    df_playoffs = df_playoffs[df_playoffs['Yr'] >= min_year]\n",
    "\n",
    "    # drop the columns we don't care about and rename the columns left\n",
    "    df_playoffs.dropna(axis=1, how='all', inplace=True)\n",
    "    df_playoffs.drop(['Lg', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 7', 'W', 'W.1', 'Favorite', 'Underdog'],\n",
    "                     inplace=True,\n",
    "                     axis=1)\n",
    "    df_playoffs.columns = ['Yr', 'Round', 'Win_Tm', 'Loss_Tm']\n",
    "\n",
    "    # remove parentheses from team names\n",
    "    df_playoffs['Win_Tm'] = df_playoffs['Win_Tm'].apply(lambda x: x[0:-4])\n",
    "    df_playoffs['Loss_Tm'] = df_playoffs['Loss_Tm'].apply(lambda x: x[0:-4])\n",
    "\n",
    "    # rename playoff series to round\n",
    "    df_playoffs['Round'] = df_playoffs['Round'].replace({\n",
    "        'Eastern Conf First Round': 0.25,\n",
    "        'Eastern Conf Semifinals': 0.5,\n",
    "        'Eastern Conf Finals': 0.75,\n",
    "        'Finals': 1,\n",
    "        'Western Conf First Round': 0.25,\n",
    "        'Western Conf Semifinals': 0.5,\n",
    "        'Western Conf Finals': 0.75,\n",
    "    })\n",
    "\n",
    "    return df_playoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df57afc",
   "metadata": {},
   "source": [
    "### Scrape One Year's Data\n",
    "Adjusting each year to have what we needed took the most cleaning. There were three different tables worth of values that we cared about: per 100 stats, advanced stats, and shooting stats. \n",
    "\n",
    "#### Loading the main datasets\n",
    "We scraped in each of the per 100, advanced, and shooting datasets with the input year and cleaned them individually. All the tables required us to drop variables that were either repetitive from other dataframes (i.e. FG, FGA, FG%) or not helpful to our model (i.e. rank, G) and then sort the teams alphabetically to allow us to merge them together. In both the advanced and shooting datasets, there were two rows of headers so we applied the first row's context to the second by adjusting the headers. In addition, both the advanced and shooting datasets had some empty columns that we dropped. Finally, the advanced data only listed total wins, but we wanted to change it to win percentage to account for years that had less games. As a last step, we merged the three dataframes together.\n",
    "\n",
    "#### Adding playoff data\n",
    "Before the year was complete we had to add in our playoff data that we scraped earlier. First, we removed some asterisks from the team names that would limit our ability to compare between datasets. Then we looped through the playoff and team dataset until we found the correct year and team matchup and saved the playoff value in the team dataset as its corresponding value in the playoff dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79cee7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_year(year, playoffs, season_url, cur_year):\n",
    "    \"\"\"\n",
    "    Scrape the years and return a dictionary of dataframes\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int\n",
    "        int containing the year to scrape from\n",
    "    playoffs : DataFrame\n",
    "        df with playoff data\n",
    "    season_url : str\n",
    "        the base url of the basketball reference link\n",
    "    cur_year : boolean\n",
    "        if the year is the current one (i.e. no playoffs)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    cur_df : DataFrame\n",
    "        temporary dataframe containing years referencing data frames\n",
    "    \"\"\"\n",
    "    # adjust the url to each year\n",
    "    url = season_url.format(year)\n",
    "\n",
    "    # read in data\n",
    "    df_p100 = pd.read_html(url, header=0, match='Per 100 Poss Stats')[0]\n",
    "    df_adv = pd.read_html(url, header=1, match='Advanced Stats')[0]\n",
    "    df_shoot = pd.read_html(url, header=1, match='Shooting Stats')[0]\n",
    "\n",
    "    # clean per 100 data\n",
    "    df_p100.drop(['Rk', 'G', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', '2P', '2PA', '2P%'],\n",
    "                 inplace=True,\n",
    "                 axis=1)\n",
    "    df_p100.sort_values(\"Team\", inplace=True)\n",
    "    df_p100.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # clean advanced data\n",
    "    df_adv['win_perc'] = round(df_adv['W'] / (df_adv['W'] + df_adv['L']), 3)\n",
    "    df_adv.drop(['Rk', 'W', 'L', 'Arena', 'Attend.'], inplace=True, axis=1)\n",
    "    df_adv.drop([30], inplace=True, axis=0)\n",
    "    df_adv.dropna(axis=1, inplace=True)\n",
    "    # rename columns in dataset\n",
    "    for i in range(14, 18):\n",
    "        df_adv.columns.values[i] = 'off_' + df_adv.columns.values[i]\n",
    "    for i in range(18, 22):\n",
    "        df_adv.columns.values[i] = 'def_' + df_adv.columns.values[i][:-2]\n",
    "    df_adv.sort_values(\"Team\", inplace=True)\n",
    "    df_adv.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # clean shooting data\n",
    "    df_shoot.drop(['Rk', 'G', 'MP'], inplace=True, axis=1)\n",
    "    df_shoot.drop([30], inplace=True, axis=0)\n",
    "    df_shoot.dropna(axis=1, inplace=True)\n",
    "    # rename columns in dataset\n",
    "    for i in range(3, 9):\n",
    "        df_shoot.columns.values[i] = 'fga_perc_' + df_shoot.columns.values[i]\n",
    "    for i in range(9, 15):\n",
    "        df_shoot.columns.values[i] = 'fg_perc_' + df_shoot.columns.values[i][:-2]\n",
    "    for i in range(15, 17):\n",
    "        df_shoot.columns.values[i] = 'fg_ast_perc_' + df_shoot.columns.values[i][:-2]\n",
    "    for i in range(17, 19):\n",
    "        df_shoot.columns.values[i] = 'dunks_' + df_shoot.columns.values[i]\n",
    "    for i in range(19, 21):\n",
    "        df_shoot.columns.values[i] = 'layups_' + df_shoot.columns.values[i][:-2]\n",
    "    for i in range(21, 23):\n",
    "        df_shoot.columns.values[i] = 'corner_' + df_shoot.columns.values[i]\n",
    "    df_shoot.columns.values[23] = 'heave_' + df_shoot.columns.values[23]\n",
    "    df_shoot.columns.values[24] = 'heave_' + df_shoot.columns.values[24][:-2]\n",
    "    df_shoot.sort_values(\"Team\", inplace=True)\n",
    "\n",
    "    # merge all the dataframes into one larger one with all the stats per team\n",
    "    cur_df = pd.merge(pd.merge(df_p100, df_adv, on='Team'), df_shoot, on='Team')\n",
    "    cur_df['Year'] = year\n",
    "\n",
    "    # narrow playoffs down to each year\n",
    "    playoffs = playoffs[playoffs['Yr'] == year]\n",
    "\n",
    "    # remove the asterix from the teams with it on the end of their name\n",
    "    for ind in cur_df.index:\n",
    "        if cur_df[\"Team\"][ind].endswith(\"*\"):\n",
    "            cur_df[\"Team\"][ind] = cur_df[\"Team\"][ind].rstrip(\"*\").strip()\n",
    "\n",
    "    # create playoff column and add values if it's not the current year\n",
    "    if not cur_year:\n",
    "        cur_df['Playoff'] = 0\n",
    "        for p_idx, p_row in playoffs.iterrows():\n",
    "            for idx, row in cur_df.iterrows():\n",
    "                if p_row['Loss_Tm'] == row['Team']:\n",
    "                    cur_df.loc[idx, 'Playoff'] = p_row['Round']\n",
    "                elif p_row['Round'] == 1 and p_row['Win_Tm'] == row['Team']:\n",
    "                    cur_df.loc[idx, 'Playoff'] = p_row['Round']\n",
    "\n",
    "    return cur_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0f8343",
   "metadata": {},
   "source": [
    "### Scrape Every Year of Data\n",
    "We took the list of years we wanted data for and looped through them with the single year scraping function. We took each dataframe and appended it to our overall dataframe to save and then returned the entire df once it was done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3825c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_years(years, playoffs, cur_year=False):\n",
    "    \"\"\"\n",
    "    Scrape the years and return a dictionary of dataframes\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    years : list\n",
    "        list containing the years to scrape\n",
    "    playoffs : DataFrame\n",
    "        df with playoff data\n",
    "    cur_year : boolean\n",
    "        if the year is the current one (i.e. no playoffs)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df : DataFrame\n",
    "        dataframe containing years referencing data frames\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    season_url = \"https://www.basketball-reference.com/leagues/NBA_{}.html\"\n",
    "\n",
    "    # loop through all the years in the list\n",
    "    for year in years:\n",
    "\n",
    "        # scrape the year's data into a dataframe\n",
    "        cur_df = scrape_year(year, playoffs, season_url, cur_year)\n",
    "\n",
    "        # add the current dataframe to our overall df of teams\n",
    "        df = pd.concat([df, cur_df], ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bc1ae1",
   "metadata": {},
   "source": [
    "### Run the Functions to Scrape the Data\n",
    "We created a list of all the years we wanted (2010 through 2022) and another list with the current year. Then we loaded in the playoff data to use with our team scraping. Finally we scraped both the full set of years from 2010 to 2022 and the current year's data and saved both as csv's to run with our ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cbd9384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list of years we want data for\n",
    "years = list(range(MIN_YEAR, MAX_YEAR + 1))\n",
    "current_year = [2023]\n",
    "\n",
    "# get the playoff data\n",
    "df_playoffs = get_playoffs(MIN_YEAR)\n",
    "\n",
    "# load in the entirety of the years seasons and save is as a csv\n",
    "df = scrape_years(years, df_playoffs)\n",
    "df.to_csv(\"seasons_data.csv\", index=False)\n",
    "\n",
    "# load in the current year with no playoff column and save as a csv\n",
    "df_23 = scrape_years(current_year, df_playoffs, cur_year=True)\n",
    "df_23.to_csv(\"2023_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
